{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E013otF9BRrJ"
   },
   "source": [
    "# **Import Libraries and Prepare Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 14102,
     "status": "ok",
     "timestamp": 1754105641085,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "uSaG6pL5NPAf",
    "outputId": "49887d29-27ee-46d0-b02d-34f80df54a3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/alicia.siahaya/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1378,
     "status": "ok",
     "timestamp": 1754105642479,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "ErwX7NjLcBI8",
    "outputId": "8061f393-930b-4d72-d366-a97253b6d4c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(302, 7)\n"
     ]
    }
   ],
   "source": [
    "url = (\n",
    "    \"https://docs.google.com/spreadsheets/d/e/2PACX-1vQx0P59eV_KGGFJe-l86dgsz9pZRehTRJoOP_tyVXbrQPtzmD97E1lZ0lgFJ-ATVTT9HkBRX5g1kRKB/pub?output=csv\"\n",
    ")\n",
    "df = pd.read_csv(url)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1754105642533,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "hR_lrZiCTZ-k",
    "outputId": "0092dc5c-eb67-4645-ae54-ec1b1b9f714d"
   },
   "outputs": [],
   "source": [
    "df_melt = df.melt(\n",
    "    value_vars=[\n",
    "        \"Apa yang Anda sukai selama masa perkuliahan Anda?\",\n",
    "        \"Apa yang Anda tidak sukai selama masa perkuliahan Anda?\"\n",
    "    ],\n",
    "    var_name='sentiment',\n",
    "    value_name='text'\n",
    ")\n",
    "\n",
    "sentiment_map = {\n",
    "    \"Apa yang Anda sukai selama masa perkuliahan Anda?\": 'positive',\n",
    "    \"Apa yang Anda tidak sukai selama masa perkuliahan Anda?\": 'negative'\n",
    "}\n",
    "df_melt['sentiment'] = df_melt['sentiment'].map(sentiment_map)\n",
    "\n",
    "df_preprocess = df_melt[['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAeW7HDvLJay"
   },
   "source": [
    "# **Data Cleansing and Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zn44zcqFLxt-"
   },
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1754105642546,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "0vAHiKQGLxE2",
    "outputId": "2041fb5a-7d15-4b3f-90e4-dc01cd8329bd"
   },
   "outputs": [],
   "source": [
    "def split_data(df, validation_data=False):\n",
    "    if validation_data:\n",
    "        train_df, val_test_df = train_test_split(\n",
    "            df, test_size=0.2, random_state=42, stratify=df['sentiment']\n",
    "        )\n",
    "        val_df, test_df = train_test_split(\n",
    "            val_test_df, test_size=0.5, random_state=42, stratify=val_test_df['sentiment']\n",
    "        )\n",
    "\n",
    "        X_train = train_df['text']\n",
    "        y_train = train_df['sentiment']\n",
    "        X_val = val_df['text']\n",
    "        y_val = val_df['sentiment']\n",
    "        X_test = test_df['text']\n",
    "        y_test = test_df['sentiment']\n",
    "\n",
    "        print(f'Training shape: {X_train.shape}, {y_train.shape}')\n",
    "        print(f'Validation shape: {X_val.shape}, {y_val.shape}')\n",
    "        print(f'Test shape: {X_test.shape}, {y_test.shape}')\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "    # Only train/test split\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, random_state=42, stratify=df['sentiment']\n",
    "    )\n",
    "    X_train = train_df['text']\n",
    "    y_train = train_df['sentiment']\n",
    "    X_test = test_df['text']\n",
    "    y_test = test_df['sentiment']\n",
    "    print(f'Training shape: {X_train.shape}, {y_train.shape}')\n",
    "    print(f'Test shape: {X_test.shape}, {y_test.shape}')\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2eCu_a7LRIi"
   },
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dictionary.normalization_dictionary import norm_dict\n",
    "\n",
    "class TextCleansing:\n",
    "    def __init__(self, text, norm_dict=None):\n",
    "        self.text = text\n",
    "        self.norm_dict = norm_dict if norm_dict else {}\n",
    "\n",
    "    def correct_typos(self, text):\n",
    "        for typo, correction in self.norm_dict.items():\n",
    "            clean_text = re.sub(\n",
    "                rf'\\b{typo}\\b', correction, text, flags=re.IGNORECASE\n",
    "            )\n",
    "        return clean_text\n",
    "    \n",
    "    def reduce_extra_characters(self, text):\n",
    "        \"\"\"\n",
    "        Contoh:\n",
    "        \"sukaaaaa\" -> \"suka\"\n",
    "        \"\"\"\n",
    "        return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    \n",
    "    def split_nya(self, text, exception_words=None):\n",
    "        if exception_words is None:\n",
    "            exception_words = [\"tanya\", \"punya\", \"bertanya\", \"hanya\"]\n",
    "\n",
    "        if text in exception_words:\n",
    "            return text\n",
    "\n",
    "        return re.sub(r'(.*?)nya$', r'\\1 nya', text)\n",
    "\n",
    "    def process_split_nya(self, text):\n",
    "        words = text.split()\n",
    "        processed_words = [\n",
    "            self.split_nya(word.strip()) for word in words\n",
    "        ]\n",
    "        return ' '.join(processed_words)\n",
    "\n",
    "    def clean(self):\n",
    "        text = self.text.lower()\n",
    "        text = re.sub(r\"[^a-zA-Z\\s']\", ' ', text)\n",
    "        text = self.process_split_nya(text)\n",
    "        text = self.correct_typos(text)\n",
    "        text = self.reduce_extra_characters(text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleansing(text):\n",
    "    tc = TextCleansing(text, norm_dict=norm_dict)\n",
    "    return tc.clean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovy8NaawLngJ"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131a5351b4ab438b82d907ade24975f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 21:52:19 INFO: Downloaded file to /Users/alicia.siahaya/stanza_resources/resources.json\n",
      "2025-12-27 21:52:19 INFO: Downloading default packages for language: id (Indonesian) ...\n",
      "2025-12-27 21:52:20 INFO: File exists: /Users/alicia.siahaya/stanza_resources/id/default.zip\n",
      "2025-12-27 21:52:24 INFO: Finished downloading models and saved to /Users/alicia.siahaya/stanza_resources\n",
      "2025-12-27 21:52:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da9c71d44a34c8abc98dd9e357c1af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 21:52:24 INFO: Downloaded file to /Users/alicia.siahaya/stanza_resources/resources.json\n",
      "2025-12-27 21:52:25 INFO: Loading these models for language: id (Indonesian):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | gsd          |\n",
      "| mwt       | gsd          |\n",
      "| pos       | gsd_charlm   |\n",
      "| lemma     | gsd_nocharlm |\n",
      "============================\n",
      "\n",
      "2025-12-27 21:52:25 INFO: Using device: cpu\n",
      "2025-12-27 21:52:25 INFO: Loading: tokenize\n",
      "2025-12-27 21:52:27 INFO: Loading: mwt\n",
      "2025-12-27 21:52:27 INFO: Loading: pos\n",
      "2025-12-27 21:52:33 INFO: Loading: lemma\n",
      "2025-12-27 21:52:33 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from src.models.preprocess.stanza_preprocessor import StanzaPreprocessor\n",
    "from src.models.preprocess.nlpid_preprocessor import NLPIdPreprocessor\n",
    "from src.dictionary.exclude_words import exclude_stopwords, exclude_lemmatization\n",
    "\n",
    "stanza_prep = StanzaPreprocessor(\n",
    "    exclude_stopwords=exclude_stopwords,\n",
    "    exclude_lemmatization=exclude_lemmatization\n",
    ")\n",
    "\n",
    "nlp_id_prep = NLPIdPreprocessor(\n",
    "    exclude_stopwords=exclude_stopwords,\n",
    "    exclude_lemmatization=exclude_lemmatization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hu8obmHcMon9"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1754105672770,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "BFAOR_psMqHz",
    "outputId": "9d6f5608-7042-420b-9c55-1725603fec28"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(df, model_type, preprocess_type=None):\n",
    "  df['cleaned_text'] = df['text'].apply(text_cleansing)\n",
    "\n",
    "  if model_type == 'svm':\n",
    "    if preprocess_type == 'nlp_id':\n",
    "      df['preprocessed_text_nlp_id'] = df['cleaned_text'].apply(nlp_id_prep.transform)\n",
    "      return df\n",
    "\n",
    "    if preprocess_type == 'stanza':\n",
    "      df['preprocessed_text_stanza'] = df['cleaned_text'].apply(stanza_prep.transform)\n",
    "      return df\n",
    "\n",
    "  if model_type == 'indobert':\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSrUd8XQMCnl"
   },
   "source": [
    "# **Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 20282,
     "status": "ok",
     "timestamp": 1754105705638,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "Lzt3MzxAMESh",
    "outputId": "6460696a-fe84-4f4b-9ab7-31c9766c8611"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import svm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, log_loss\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYt87GmoMA1L"
   },
   "source": [
    "# **PIPELINE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 20999,
     "status": "ok",
     "timestamp": 1754105726639,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "3rbZKUfPP4Eg",
    "outputId": "05ea14ce-21d2-4a1e-f1b7-0cca8cd02b0f"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('src/models/svm_model/vectorizer1.pkl', 'rb') as f:\n",
    "    vectorizer1 = pickle.load(f)\n",
    "\n",
    "with open('src/models/svm_model/model_svm1.pkl', 'rb') as f:\n",
    "    model1 = pickle.load(f)\n",
    "\n",
    "with open('src/models/svm_model/vectorizer2.pkl', 'rb') as f:\n",
    "    vectorizer2 = pickle.load(f)\n",
    "\n",
    "with open('src/models/svm_model/model_svm2.pkl', 'rb') as f:\n",
    "    model2 = pickle.load(f)\n",
    "\n",
    "model_indobert = AutoModelForSequenceClassification.from_pretrained(\"src/models/indobert_model\")\n",
    "tokenizer_indobert = AutoTokenizer.from_pretrained(\"src/models/indobert_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1754105726650,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "J_MYk2vmL71H",
    "outputId": "522bb85c-47ac-46db-f357-bbf11db51b3f"
   },
   "outputs": [],
   "source": [
    "def run_svm_pipeline(df, model_type, vectorizer=None, model=None, preprocess_type=None):\n",
    "  start_time = time.time()\n",
    "  df_preprocessed = prepare_dataset(df, model_type=model_type, preprocess_type=preprocess_type)\n",
    "\n",
    "  df_tfidf = vectorizer.transform(df_preprocessed[f'preprocessed_text_{preprocess_type}'])\n",
    "  tfidf_test_df = pd.DataFrame(df_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "  y_pred = model.predict(tfidf_test_df)\n",
    "\n",
    "  end_time = time.time()\n",
    "  execution_time = end_time - start_time\n",
    "  print(f\"Execution time for {model_type} using {preprocess_type}: {execution_time} seconds\")\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1754106449254,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "SfzEiZrQWkjb",
    "outputId": "c0de29ff-381a-4e56-b8d4-f910b1347ce7"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "def run_indobert_pipeline(df, model, tokenizer):\n",
    "  start_time = time.time()\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  df_preprocessed = prepare_dataset(df, model_type='indobert')\n",
    "  texts = df_preprocessed['cleaned_text'].tolist()\n",
    "\n",
    "  cleaned_texts = [text_cleansing(t) for t in texts]\n",
    "  inputs = tokenizer(cleaned_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(**inputs)\n",
    "      preds = torch.argmax(outputs.logits, dim=1)\n",
    "      labels = [id2label[int(p)] for p in preds]\n",
    "\n",
    "  end_time = time.time()\n",
    "  execution_time = end_time - start_time\n",
    "  print(f\"Execution time for IndoBERT: {execution_time} seconds\")\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihYDvPrrZZzV"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1754106470677,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "JNAXXrOL5Jh8",
    "outputId": "65114754-2b44-4117-86f8-6c0a3fac2cce"
   },
   "outputs": [],
   "source": [
    "label_map = {'positive':1, 'negative':0}\n",
    "id2label = {v: k for k, v in label_map.items()}\n",
    "target_names = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtcrPoet365y"
   },
   "source": [
    "### **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 14217,
     "status": "ok",
     "timestamp": 1754106103745,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "6a30c25e",
    "outputId": "43fd4fad-bb08-4eae-c8f6-7eaa14e022f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (483,), (483,)\n",
      "Test shape: (121,), (121,)\n",
      "Execution time for svm using nlp_id: 0.3294408321380615 seconds\n",
      "Execution time for svm using stanza: 40.16508221626282 seconds\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_data(df_preprocess, validation_data=False)\n",
    "y_train = y_train.map(label_map)\n",
    "y_test = y_test.map(label_map)\n",
    "\n",
    "df_test = pd.DataFrame({'text': X_test, 'sentiment': y_test})\n",
    "\n",
    "y_pred_test_nlp_id = run_svm_pipeline(df_test, model_type='svm', vectorizer=vectorizer1, model=model1, preprocess_type='nlp_id')\n",
    "y_pred_test_stanza = run_svm_pipeline(df_test, model_type='svm', vectorizer=vectorizer2, model=model2, preprocess_type='stanza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1754106140879,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "vMqmSPgi06Hg",
    "outputId": "42346cdb-e7ad-49a3-e286-f4112ebe7868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for SVM with nlp_id:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.93      0.93        61\n",
      "    positive       0.93      0.92      0.92        60\n",
      "\n",
      "    accuracy                           0.93       121\n",
      "   macro avg       0.93      0.93      0.93       121\n",
      "weighted avg       0.93      0.93      0.93       121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report for SVM with nlp_id:\")\n",
    "print(classification_report(y_test, y_pred_test_nlp_id, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1754106143748,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "ZVKjvw_p1N6I",
    "outputId": "447b04ec-fd90-4268-c579-f53ef74b3677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for SVM with stanza:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.97      0.97      0.97        61\n",
      "    positive       0.97      0.97      0.97        60\n",
      "\n",
      "    accuracy                           0.97       121\n",
      "   macro avg       0.97      0.97      0.97       121\n",
      "weighted avg       0.97      0.97      0.97       121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report for SVM with stanza:\")\n",
    "print(classification_report(y_test, y_pred_test_stanza, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mntcYUVU39Qu"
   },
   "source": [
    "### **IndoBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 12050,
     "status": "ok",
     "timestamp": 1754106485518,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "N3GaCqHAWpvo",
    "outputId": "310c7cad-9954-402d-8b0b-3a4e5cd2a6e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (483,), (483,)\n",
      "Validation shape: (60,), (60,)\n",
      "Test shape: (61,), (61,)\n",
      "Execution time for IndoBERT: 5.943676948547363 seconds\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test = split_data(df_preprocess, validation_data=True)\n",
    "\n",
    "df_test_indobert = pd.DataFrame({'text': x_test, 'sentiment': y_test})\n",
    "\n",
    "y_pred_indobert = run_indobert_pipeline(df_test_indobert, model_indobert, tokenizer_indobert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1754106513815,
     "user": {
      "displayName": "alicia siahaya",
      "userId": "00095746084346106118"
     },
     "user_tz": -420
    },
    "id": "-QIam533UzJ6",
    "outputId": "4df98b63-8157-445c-d386-65031248f88e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for IndoBERT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.97      1.00      0.98        31\n",
      "    positive       1.00      0.97      0.98        30\n",
      "\n",
      "    accuracy                           0.98        61\n",
      "   macro avg       0.98      0.98      0.98        61\n",
      "weighted avg       0.98      0.98      0.98        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report for IndoBERT:\")\n",
    "print(classification_report(y_test, y_pred_indobert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVILBz4f5Zbb"
   },
   "source": [
    "# **Summary**\n",
    "\n",
    "## **SVM**\n",
    "SVM model was evaluated using two different text preprocessing methods in combination with a TF-IDF Vectorizer:\n",
    "1.  nlp_id for lemmatization and stopwords removal -> achieved **93%** accuracy\n",
    "2.  stanza for lemmatization and Sastrawi for stopwords removal -> achieved **97%** accuracy\n",
    "\n",
    "Although the accuracy difference between the two SVM models were relatively small, we will continue to observe based on the time execution for inference. SVM model with nlp_id completed the inference in under 1 second, whereas the SVM model with Stanza and Sastrawi required 40 seconds for the same task.\n",
    "\n",
    "Considering both accuracy and time efficiency, the SVM model with nlp_id is more suitable for practical and public use, as it offers significantly fast prediction results with only a 4% decrease in accuracy compared to the Stanza-Sastrawi based approach.\n",
    "\n",
    "## **IndoBERT**\n",
    "The IndoBERT model achieved highest performance among all evaluated model, with **98%** accuracy, which is expected given its transformer based architecture and larger model capacity. Despite its longer computational time compared to SVM model, IndoBERT showed efficient inference requiring only 5 seconds.\n",
    "\n",
    "## **Conclusion**\n",
    "Overall, IndoBERT provides the best trade-off between accuracy and inference time, excelling in performance while maintaining a fast computational time. Therefore, IndoBERT is selected as the final model for the sentiment analysis. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyONYTTys1E+17AQ+sNYcjxQ",
   "gpuType": "T4",
   "mount_file_id": "1P30spYHn7omM3IADOTfyxCJcPV31stq_",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "thesis_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
