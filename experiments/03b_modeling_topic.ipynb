{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a0bdf0",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e150dcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\BCA\\BERTopic\\runs\\bertopic_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "\n",
    "import nlp_id\n",
    "from nlp_id.lemmatizer import Lemmatizer\n",
    "from nlp_id.tokenizer import Tokenizer\n",
    "from nlp_id.stopword import StopWord\n",
    "import stanza\n",
    "import Sastrawi\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, Alignment, PatternFill\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d8ffc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA NOT Available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d79887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5c6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.chdir(\"C:/BCA/BERTopic/runs\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa2a3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cleaned_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "dce5b688-7241-40b3-b680-c427ac5c3546",
       "rows": [
        [
         "0",
         "saya suka materi yang sudah disiapkan oleh pihak kampus karena memudahkan mahasiswa saya juga menyukai program enrichment yang disediakan kampus saya sehingga mahasiswa dapat belajar di ruang lingkup yang lebih luas",
         "1"
        ],
        [
         "1",
         "bisa bertemu dengan teman teman baru dan mendapatkan koneksi serta mendapatkan pelajaran yang berguna bagi saya kedepan nya",
         "1"
        ],
        [
         "2",
         "saya suka dengan makanan yang ada di dalam kampus saya terutama bakmi efata selain itu disekitar kampus juga banyak makanan enak",
         "1"
        ],
        [
         "3",
         "fasilitas kampus alam sutera sangat bagus pelajaran lab diajarkan oleh asisten yang sangat mengerti materi",
         "1"
        ],
        [
         "4",
         "saya suka dengan pertemanan nya solid mau saling bantu satu sama lain bagi bagi kisi kisi pas ujian terus saling ngajarin",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saya suka materi yang sudah disiapkan oleh pihak kampus karena memudahkan mahasiswa saya juga menyukai program enrichment yang disediakan kampus saya sehingga mahasiswa dapat belajar di ruang lingkup yang lebih luas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bisa bertemu dengan teman teman baru dan mendapatkan koneksi serta mendapatkan pelajaran yang berguna bagi saya kedepan nya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saya suka dengan makanan yang ada di dalam kampus saya terutama bakmi efata selain itu disekitar kampus juga banyak makanan enak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fasilitas kampus alam sutera sangat bagus pelajaran lab diajarkan oleh asisten yang sangat mengerti materi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saya suka dengan pertemanan nya solid mau saling bantu satu sama lain bagi bagi kisi kisi pas ujian terus saling ngajarin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                              cleaned_text  \\\n",
       "0  saya suka materi yang sudah disiapkan oleh pihak kampus karena memudahkan mahasiswa saya juga menyukai program enrichment yang disediakan kampus saya sehingga mahasiswa dapat belajar di ruang lingkup yang lebih luas   \n",
       "1                                                                                              bisa bertemu dengan teman teman baru dan mendapatkan koneksi serta mendapatkan pelajaran yang berguna bagi saya kedepan nya   \n",
       "2                                                                                         saya suka dengan makanan yang ada di dalam kampus saya terutama bakmi efata selain itu disekitar kampus juga banyak makanan enak   \n",
       "3                                                                                                               fasilitas kampus alam sutera sangat bagus pelajaran lab diajarkan oleh asisten yang sangat mengerti materi   \n",
       "4                                                                                                saya suka dengan pertemanan nya solid mau saling bantu satu sama lain bagi bagi kisi kisi pas ujian terus saling ngajarin   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modeling = pd.read_csv('src/data/df_modeling_BERT.csv')\n",
    "df_modeling['sentiment'] = df_modeling['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "df_modeling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef63ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dictionary.exclude_words import exclude_stopwords\n",
    "\n",
    "stopword = StopWord()\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Menambahkan kata untuk stop words\n",
    "stop_words = stopword.get_stopword()\n",
    "stop_words.append(exclude_stopwords)\n",
    "\n",
    "def clean_text(text, negation=True):\n",
    "  if negation:\n",
    "      for phrase in ['sangat tidak menyukai', 'tidak menyukai','sangat tidak suka', 'tidak suka', 'kurang suka', 'kurang menyukai', 'ga suka', 'gak suka', 'ga menyukai', 'gak menyukai']:\n",
    "          text = text.replace(phrase, '')\n",
    "\n",
    "  for phrase in ['suka', 'sangat suka', 'menyukai', 'sangat menyukai']:\n",
    "      text = text.replace(phrase, '')\n",
    "\n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "  return text\n",
    "\n",
    "def text_preprocessing(text):\n",
    "\n",
    "  # # Tokenisasi menggunakan Tokenizer dari nlp_id\n",
    "  tokens = tokenizer.tokenize(text)\n",
    "\n",
    "  filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "  # Menghapus spasi yang berlebih\n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "  processed_text = \" \".join(filtered_tokens)\n",
    "\n",
    "  return processed_text\n",
    "\n",
    "def prepare_dataset(df_modeling):\n",
    "    df_pos = df_modeling[df_modeling['sentiment'] == 1].copy()\n",
    "    df_neg = df_modeling[df_modeling['sentiment'] == 0].copy()\n",
    "\n",
    "    df_pos['cleaned_text'] = df_pos['cleaned_text'].apply(clean_text, negation=False)\n",
    "    df_neg['cleaned_text'] = df_neg['cleaned_text'].apply(clean_text, negation=True)\n",
    "\n",
    "    df_pos['processed_text'] = df_pos['cleaned_text'].apply(text_preprocessing)\n",
    "    df_neg['processed_text'] = df_neg['cleaned_text'].apply(text_preprocessing)\n",
    "\n",
    "    text_pos = df_pos['processed_text'].astype(str).tolist()\n",
    "    text_neg = df_neg['processed_text'].astype(str).tolist()\n",
    "\n",
    "    return text_pos, text_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2746c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_pos, texts_neg = prepare_dataset(df_modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99edb4",
   "metadata": {},
   "source": [
    "## Predict New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2079c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_predict_topics(model_path, df, text_column, embeddings):\n",
    "    \"\"\"Load BERTopic model and predict topics for given dataframe\"\"\"\n",
    "    model = BERTopic.load(model_path)\n",
    "    docs = df[text_column].tolist()\n",
    "\n",
    "    print(\"Has embedding model:\", hasattr(model, \"embedding_model\"))\n",
    "    print(\"UMAP expects n_features:\", getattr(model.umap_model, \"n_features_in_\", \"Unknown\"))\n",
    "    print(dir(model.umap_model))\n",
    "    topics, probs = model.transform(docs, embeddings)\n",
    "\n",
    "    df_pred = df.copy()\n",
    "    df_pred[\"topic\"] = topics\n",
    "    df_pred[\"topic_proba\"] = probs\n",
    "\n",
    "    topic_info = model.get_topic_info()\n",
    "    df_pred = df_pred.merge(\n",
    "        topic_info[[\"Topic\", \"Name\"]],\n",
    "        left_on=\"topic\",\n",
    "        right_on=\"Topic\",\n",
    "        how=\"left\"\n",
    "    ).drop(columns=[\"Topic\"]).rename(columns={\"Name\": \"topic_name\"})\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c81b4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topics_by_sentiment(df, positive_model_path, negative_model_path, text_column='processed_text'):\n",
    "    \"\"\"\n",
    "    Predict topics for both positive and negative sentiment data using respective models\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with sentiment column (1 for positive, 0 for negative)\n",
    "        positive_model_path: Path to the positive sentiment BERTopic model\n",
    "        negative_model_path: Path to the negative sentiment BERTopic model\n",
    "        text_column: Column name containing the text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with topic predictions for both sentiments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate positive and negative data\n",
    "    df_pos = df[df['sentiment'] == 1].copy().reset_index()  # Preserve index as a column\n",
    "    df_neg = df[df['sentiment'] == 0].copy().reset_index()  # Preserve index as a column\n",
    "    \n",
    "    print(f\"Processing {len(df_pos)} positive sentiment texts...\")\n",
    "    print(f\"Processing {len(df_neg)} negative sentiment texts...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process positive sentiment data\n",
    "    if len(df_pos) > 0:\n",
    "        print(\"\\n--- Processing Positive Sentiment Data ---\")\n",
    "        embedding_model_pos = SentenceTransformer(\"indobenchmark/indobert-base-p1\")\n",
    "        embedding_pos = embedding_model_pos.encode(df_pos[text_column].tolist(), show_progress_bar=True)\n",
    "        df_pos_pred = load_and_predict_topics(\n",
    "            model_path=positive_model_path,\n",
    "            df=df_pos,\n",
    "            text_column=text_column,\n",
    "            embeddings=embedding_pos\n",
    "        )\n",
    "        df_pos_pred['sentiment_type'] = 'positive'\n",
    "        results.append(df_pos_pred)\n",
    "        print(f\"Positive topics found: {df_pos_pred['topic'].nunique()}\")\n",
    "        print(\"Topic distribution (positive):\")\n",
    "        print(df_pos_pred['topic_name'].value_counts().head())\n",
    "    \n",
    "    # Process negative sentiment data\n",
    "    if len(df_neg) > 0:\n",
    "        print(\"\\n--- Processing Negative Sentiment Data ---\")\n",
    "        embedding_model_neg = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        embedding_neg = embedding_model_neg.encode(df_neg[text_column].tolist(), show_progress_bar=True)\n",
    "        df_neg_pred = load_and_predict_topics(\n",
    "            model_path=negative_model_path,\n",
    "            df=df_neg,\n",
    "            text_column=text_column,\n",
    "            embeddings=embedding_neg\n",
    "        )\n",
    "        df_neg_pred['sentiment_type'] = 'negative'\n",
    "        results.append(df_neg_pred)\n",
    "        print(f\"Negative topics found: {df_neg_pred['topic'].nunique()}\")\n",
    "        print(\"Topic distribution (negative):\")\n",
    "        print(df_neg_pred['topic_name'].value_counts().head())\n",
    "    \n",
    "    # Combine results\n",
    "    if results:\n",
    "        df_combined = pd.concat(results, ignore_index=False)  # Preserve index\n",
    "        # Sort by the preserved 'index' column\n",
    "        df_combined = df_combined.sort_values('index')\n",
    "        return df_combined\n",
    "    else:\n",
    "        print(\"No data to process!\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4af990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(\n",
    "    'src/data/synthetic_100_reviews.csv'\n",
    ")\n",
    "df_new['sentiment'] = df_new['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "new_texts_pos, new_texts_neg = prepare_dataset(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0089c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the dataframe\n",
    "df_new_processed = df_new.copy()\n",
    "\n",
    "# Apply same preprocessing logic as in prepare_dataset\n",
    "df_pos_temp = df_new_processed[df_new_processed['sentiment'] == 1].copy()\n",
    "df_neg_temp = df_new_processed[df_new_processed['sentiment'] == 0].copy()\n",
    "\n",
    "if len(df_pos_temp) > 0:\n",
    "    df_pos_temp['cleaned_text'] = df_pos_temp['cleaned_text'].apply(clean_text, negation=False)\n",
    "    df_pos_temp['processed_text'] = df_pos_temp['cleaned_text'].apply(text_preprocessing)\n",
    "\n",
    "if len(df_neg_temp) > 0:\n",
    "    df_neg_temp['cleaned_text'] = df_neg_temp['cleaned_text'].apply(clean_text, negation=True)\n",
    "    df_neg_temp['processed_text'] = df_neg_temp['cleaned_text'].apply(text_preprocessing)\n",
    "\n",
    "# Combine back\n",
    "df_new_processed = pd.concat([df_pos_temp, df_neg_temp], ignore_index=True) if len(df_pos_temp) > 0 and len(df_neg_temp) > 0 else (df_pos_temp if len(df_pos_temp) > 0 else df_neg_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4e84e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STARTING TOPIC PREDICTION\n",
      "==================================================\n",
      "Processing 46 positive sentiment texts...\n",
      "Processing 54 negative sentiment texts...\n",
      "\n",
      "--- Processing Positive Sentiment Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name indobenchmark/indobert-base-p1. Creating a new one with mean pooling.\n",
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has embedding model: True\n",
      "UMAP expects n_features: Unknown\n",
      "['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__sklearn_clone__', '__sklearn_tags__', '__str__', '__sub__', '__subclasshook__', '__weakref__', '_a', '_b', '_build_request_for_signature', '_check_custom_metric', '_check_feature_names', '_check_n_features', '_densmap_kwds', '_disconnection_distance', '_doc_link_module', '_doc_link_template', '_doc_link_url_param_generator', '_fit_embed_data', '_get_default_requests', '_get_doc_link', '_get_metadata_request', '_get_param_names', '_get_tags', '_initial_alpha', '_input_distance_func', '_input_hash', '_inverse_distance_func', '_metric_kwds', '_more_tags', '_n_features_out', '_n_neighbors', '_original_n_threads', '_output_distance_func', '_output_metric_kwds', '_populate_combined_params', '_raw_data', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_rhos', '_sigmas', '_small_data', '_sparse_data', '_supervised', '_target_metric_kwds', '_validate_data', '_validate_parameters', '_validate_params', 'a', 'angular_rp_forest', 'b', 'dens_frac', 'dens_lambda', 'dens_var_shift', 'densmap', 'disconnection_distance', 'embedding_', 'fit', 'fit_transform', 'force_approximation_algorithm', 'get_feature_names_out', 'get_metadata_routing', 'get_params', 'graph_', 'graph_dists_', 'init', 'inverse_transform', 'knn_dists', 'knn_indices', 'knn_search_index', 'learning_rate', 'local_connectivity', 'low_memory', 'metric', 'metric_kwds', 'min_dist', 'n_components', 'n_epochs', 'n_epochs_list', 'n_jobs', 'n_neighbors', 'negative_sample_rate', 'output_dens', 'output_metric', 'output_metric_kwds', 'precomputed_knn', 'random_state', 'repulsion_strength', 'set_fit_request', 'set_op_mix_ratio', 'set_params', 'set_transform_request', 'spread', 'target_metric', 'target_metric_kwds', 'target_n_neighbors', 'target_weight', 'tqdm_kwds', 'transform', 'transform_mode', 'transform_queue_size', 'transform_seed', 'unique', 'update', 'verbose']\n",
      "Positive topics found: 2\n",
      "Topic distribution (positive):\n",
      "topic_name\n",
      "-1_artikel_menulis_orang_teman         43\n",
      "2_peluang_binus_mahasiswa_indonesia     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Processing Negative Sentiment Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has embedding model: True\n",
      "UMAP expects n_features: Unknown\n",
      "['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__sklearn_clone__', '__sklearn_tags__', '__str__', '__sub__', '__subclasshook__', '__weakref__', '_a', '_b', '_build_request_for_signature', '_check_custom_metric', '_check_feature_names', '_check_n_features', '_densmap_kwds', '_disconnection_distance', '_doc_link_module', '_doc_link_template', '_doc_link_url_param_generator', '_fit_embed_data', '_get_default_requests', '_get_doc_link', '_get_metadata_request', '_get_param_names', '_get_tags', '_initial_alpha', '_input_distance_func', '_input_hash', '_inverse_distance_func', '_metric_kwds', '_more_tags', '_n_features_out', '_n_neighbors', '_original_n_threads', '_output_distance_func', '_output_metric_kwds', '_populate_combined_params', '_raw_data', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_rhos', '_sigmas', '_small_data', '_sparse_data', '_supervised', '_target_metric_kwds', '_validate_data', '_validate_parameters', '_validate_params', 'a', 'angular_rp_forest', 'b', 'dens_frac', 'dens_lambda', 'dens_var_shift', 'densmap', 'disconnection_distance', 'embedding_', 'fit', 'fit_transform', 'force_approximation_algorithm', 'get_feature_names_out', 'get_metadata_routing', 'get_params', 'graph_', 'graph_dists_', 'init', 'inverse_transform', 'knn_dists', 'knn_indices', 'knn_search_index', 'learning_rate', 'local_connectivity', 'low_memory', 'metric', 'metric_kwds', 'min_dist', 'n_components', 'n_epochs', 'n_epochs_list', 'n_jobs', 'n_neighbors', 'negative_sample_rate', 'output_dens', 'output_metric', 'output_metric_kwds', 'precomputed_knn', 'random_state', 'repulsion_strength', 'set_fit_request', 'set_op_mix_ratio', 'set_params', 'set_transform_request', 'spread', 'target_metric', 'target_metric_kwds', 'target_n_neighbors', 'target_weight', 'tqdm_kwds', 'transform', 'transform_mode', 'transform_queue_size', 'transform_seed', 'unique', 'update', 'verbose']\n",
      "Negative topics found: 4\n",
      "Topic distribution (negative):\n",
      "topic_name\n",
      "0_kampus_dosen_mahasiswa_fasilitas    38\n",
      "2_toilet_tisu_fasilitas_kampus         8\n",
      "-1_tpilet_serinh_tong_dryer            7\n",
      "1_tissue_pressure_toilet_grinding      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "PREDICTION RESULTS SUMMARY\n",
      "==================================================\n",
      "Total predictions: 100\n",
      "Positive sentiment: 46\n",
      "Negative sentiment: 54\n",
      "\n",
      "Overall topic distribution:\n",
      "  sentiment_type                           topic_name  count\n",
      "4       positive       -1_artikel_menulis_orang_teman     43\n",
      "1       negative   0_kampus_dosen_mahasiswa_fasilitas     38\n",
      "3       negative       2_toilet_tisu_fasilitas_kampus      8\n",
      "0       negative          -1_tpilet_serinh_tong_dryer      7\n",
      "5       positive  2_peluang_binus_mahasiswa_indonesia      3\n",
      "2       negative    1_tissue_pressure_toilet_grinding      1\n",
      "\n",
      "Sample predictions:\n",
      "                                                                                                                                                                                    cleaned_text  \\\n",
      "0   saya bertemu lokasi orang baru karena bisa memberikan informasi baru dan membantu dalam tugas dan juga kampus dekat dengan dengan rumah karena membantu dalam biaya seharian dan bantu waktu   \n",
      "1                                                                    saya dengan juga yang ada di dalam kampus saya terutama bakmi efata selain itu disekitar kampus makanan banyak makanan enak   \n",
      "2                                                                           lingkungan saya pertemanan karena sangat inklusif dan supportif sehingga saya bisa berkembang dan menjadi lebih baik   \n",
      "3  saya kelas di uph karena terkesan santai namun materi nya tetap posisi dengan baik saya juga kantin nya yang memiliki banyak pilihan makanan serta tersampaikan kampus yang dekat dengan mall   \n",
      "4                                                                                saya dengan teman dan tentang topik yang dari pelajari saya juga dengan lingkungan kampus yang dekat saya rumah   \n",
      "5                  saya lokasi kampus yang lumayan dekat dengan oleh saya rumah karena itu saya tidak perlu mengekos atau melakukan perjalanan yang jauh menggunakan kendaraan pribadi atau umum   \n",
      "6                                                        saya beberapa dosen karena kampus di dapat memberikan penjelasan yang jelas dan mudah dipahami saya juga teman teman yang ada di kampus   \n",
      "7                                                                                saya metode pleno di kampus saya seperti sgd dan pembelajaran saya juga menikmati kegiatan ekstra yang diadakan   \n",
      "8                                                                                            saya teman makan di lingkungan saya karena bisa belajar bareng teman bareng terutama saat di kampus   \n",
      "9                                                                                        saya area kampus nya dan area di dalam kampus karena banyak fasilitas hijau nya dan banyak makanan juga   \n",
      "\n",
      "  sentiment_type  topic                           topic_name  topic_proba  \n",
      "0       positive     -1       -1_artikel_menulis_orang_teman     0.957083  \n",
      "1       positive      2  2_peluang_binus_mahasiswa_indonesia     1.000000  \n",
      "2       positive     -1       -1_artikel_menulis_orang_teman     0.000000  \n",
      "3       positive     -1       -1_artikel_menulis_orang_teman     0.890193  \n",
      "4       positive     -1       -1_artikel_menulis_orang_teman     0.000000  \n",
      "5       positive      2  2_peluang_binus_mahasiswa_indonesia     1.000000  \n",
      "6       positive     -1       -1_artikel_menulis_orang_teman     1.000000  \n",
      "7       positive     -1       -1_artikel_menulis_orang_teman     0.860841  \n",
      "8       positive     -1       -1_artikel_menulis_orang_teman     0.344758  \n",
      "9       positive      2  2_peluang_binus_mahasiswa_indonesia     0.374300  \n",
      "\n",
      "Results saved to: results/topic_predictions.csv\n",
      "\n",
      "==============================\n",
      "DETAILED ANALYSIS\n",
      "==============================\n",
      "\n",
      "Top 5 topics for POSITIVE sentiment:\n",
      "  -1_artikel_menulis_orang_teman: 43 documents\n",
      "  2_peluang_binus_mahasiswa_indonesia: 3 documents\n",
      "\n",
      "Top 5 topics for NEGATIVE sentiment:\n",
      "  0_kampus_dosen_mahasiswa_fasilitas: 38 documents\n",
      "  2_toilet_tisu_fasilitas_kampus: 8 documents\n",
      "  -1_tpilet_serinh_tong_dryer: 7 documents\n",
      "  1_tissue_pressure_toilet_grinding: 1 documents\n",
      "\n",
      "Average topic confidence:\n",
      "  negative: 0.591\n",
      "  positive: 0.540\n"
     ]
    }
   ],
   "source": [
    "# Predict topics using respective models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING TOPIC PREDICTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "df_predictions = predict_topics_by_sentiment(\n",
    "    df=df_new_processed,\n",
    "    positive_model_path=\"src/model/bertopic/best_positive_model\",\n",
    "    negative_model_path=\"src/model/bertopic/best_negative_model\",\n",
    "    text_column='processed_text'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTION RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(df_predictions) > 0:\n",
    "    print(f\"Total predictions: {len(df_predictions)}\")\n",
    "    print(f\"Positive sentiment: {len(df_predictions[df_predictions['sentiment'] == 1])}\")\n",
    "    print(f\"Negative sentiment: {len(df_predictions[df_predictions['sentiment'] == 0])}\")\n",
    "    \n",
    "    print(\"\\nOverall topic distribution:\")\n",
    "    topic_dist = df_predictions.groupby(['sentiment_type', 'topic_name']).size().reset_index(name='count')\n",
    "    print(topic_dist.sort_values('count', ascending=False))\n",
    "    \n",
    "    print(\"\\nSample predictions:\")\n",
    "    print(df_predictions[['cleaned_text', 'sentiment_type', 'topic', 'topic_name', 'topic_proba']].head(10))\n",
    "    \n",
    "    # Save results\n",
    "    output_path = 'results/topic_predictions.csv'\n",
    "    df_predictions.to_csv(output_path, index=False)\n",
    "    print(f\"\\nResults saved to: {output_path}\")\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"DETAILED ANALYSIS\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Top topics for each sentiment\n",
    "    print(\"\\nTop 5 topics for POSITIVE sentiment:\")\n",
    "    pos_topics = df_predictions[df_predictions['sentiment'] == 1]['topic_name'].value_counts().head()\n",
    "    for topic, count in pos_topics.items():\n",
    "        print(f\"  {topic}: {count} documents\")\n",
    "    \n",
    "    print(\"\\nTop 5 topics for NEGATIVE sentiment:\")\n",
    "    neg_topics = df_predictions[df_predictions['sentiment'] == 0]['topic_name'].value_counts().head()\n",
    "    for topic, count in neg_topics.items():\n",
    "        print(f\"  {topic}: {count} documents\")\n",
    "    \n",
    "    # Average topic probability by sentiment\n",
    "    print(f\"\\nAverage topic confidence:\")\n",
    "    avg_proba = df_predictions.groupby('sentiment_type')['topic_proba'].mean()\n",
    "    for sentiment, prob in avg_proba.items():\n",
    "        print(f\"  {sentiment}: {prob:.3f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No predictions generated. Please check your data and model paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb20de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
